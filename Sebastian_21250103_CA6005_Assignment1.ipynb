{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f004d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDublin City University\\nMSc Computer Science (Artificial Intelligence)\\nCA6005 2022-2023 Mechanics of Search\\nAssignment 1: Search Engine\\nStudent name: Thomas Sebastian\\nStudent ID: 21250103 (University of Galway, Ireland)\\nemail address: thomas.sebastian3@mail.dcu.ie\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dublin City University\n",
    "MSc Computer Science (Artificial Intelligence)\n",
    "CA6005 2022-2023 Mechanics of Search\n",
    "Assignment 1: Search Engine\n",
    "Student name: Thomas Sebastian\n",
    "Student ID: 21250103 (University of Galway, Ireland)\n",
    "email address: thomas.sebastian3@mail.dcu.ie\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deded69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders necessary for storing parsed and processed text. Delete pre-existing folders\n",
    "import os\n",
    "folder_paths = [\"cranfieldDocs\", \"preprocessed_cranfieldDocs\", \"cranfieldQueries\", \"preprocessed_cranfieldQueries\"]\n",
    "for folder_path in folder_paths:\n",
    "    # Check if folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # Delete folder and its contents\n",
    "        os.system(\"rm -rf \" + folder_path)\n",
    "        # Create folder\n",
    "        os.makedirs(folder_path)\n",
    "    # Create a directory to store the output files\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)       \n",
    "\n",
    "# Delete 'resultsCosineSimilarity.txt' file\n",
    "if os.path.exists(\"resultsCosineSimilarity.txt\"):\n",
    "    os.remove(\"resultsCosineSimilarity.txt\")\n",
    "\n",
    "# Delete 'resultsBM25.txt' file\n",
    "if os.path.exists(\"resultsBM25.txt\"):\n",
    "    os.remove(\"resultsBM25.txt\")    \n",
    "    \n",
    "# Delete 'resultsGensim.txt' file\n",
    "if os.path.exists(\"resultsGensim.txt\"):\n",
    "    os.remove(\"resultsGensim.txt\")  \n",
    "    \n",
    "# Delete Evaluation files if they exist\n",
    "if os.path.exists(\"cosine_eval.txt\"):\n",
    "    os.remove(\"cosine_eval.txt\")\n",
    "\n",
    "if os.path.exists(\"bm25_eval.txt\"):\n",
    "    os.remove(\"bm25_eval.txt\")\n",
    "    \n",
    "if os.path.exists(\"gensim_eval.txt\"):\n",
    "    os.remove(\"gensim_eval.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d87f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Create a directory to store the query files\n",
    "os.makedirs(\"cranfieldQueries\", exist_ok=True)\n",
    "\n",
    "# Load the Query XML file\n",
    "tree = ET.parse(\"cran.qry.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract the queries with tag \"top\"\n",
    "queries = root.findall(\"top\")\n",
    "\n",
    "# Process each query, specifically information in tags \"num\" and \"title\"\n",
    "# which contain the query number and the text for query\n",
    "for query in queries:\n",
    "    # Extract the query number and title\n",
    "    query_num = query.find(\"num\").text\n",
    "    query_title = query.find(\"title\").text\n",
    "    \n",
    "    # Save the query title to a file for pre-processing\n",
    "    filename = os.path.join(\"cranfieldQueries\", f\"{query_num}\")\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(query_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97962ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Load the cran.all.1400 XML fil\n",
    "with open('cran.all.1400.xml', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Parse the XML file using Beautiful Soup by creating an BS object\n",
    "soup = BeautifulSoup(data, 'xml')\n",
    "\n",
    "# Extract the docno and text elements from each document and save to a new file\n",
    "# with tag \"doc\"\n",
    "for doc in soup.find_all('doc'):\n",
    "    docno = doc.docno.string.strip()\n",
    "\n",
    "    # Create a new file for the document and write text into the same file name\n",
    "    with open(f'cranfieldDocs/{docno}', 'w') as file:\n",
    "        file.write(str(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06f5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#file path for original Cranfield docs\n",
    "raw_file_path = 'cranfieldDocs'\n",
    "#file path for pre processed docs in cranfield documennts set\n",
    "preprocessed_file_path = 'preprocessed_cranfieldDocs'\n",
    "\n",
    "#create folder for preprocessed files\n",
    "if os.path.isdir(preprocessed_file_path):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(preprocessed_file_path)    \n",
    "    \n",
    "#Get list of all docs in the Cranfield database\n",
    "file_names = os.listdir(raw_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a897afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create a directory to store the preprocessed queryfiles\n",
    "os.makedirs(\"preprocessed_cranfieldQueries\", exist_ok=True)\n",
    "\n",
    "# Load the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize the lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Process each file in the cranfieldQueries folder\n",
    "for filename in os.listdir(\"cranfieldQueries\"):\n",
    "    # Read the contents of the file\n",
    "    with open(os.path.join(\"cranfieldQueries\", filename), 'r') as file:\n",
    "        contents = file.read()\n",
    "    \n",
    "    # Preprocess the contents\n",
    "    contents = contents.replace(\".\", \"\")\n",
    "    contents = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', contents)\n",
    "    contents = word_tokenize(contents.lower())\n",
    "    contents = [word for word in contents if word not in stop_words]\n",
    "    contents = [lemmatizer.lemmatize(word) for word in contents]\n",
    "    contents = [stemmer.stem(word) for word in contents]\n",
    "    \n",
    "    # Save the preprocessed contents to a file with the same filename in the preprocessed_cranfieldQueries folder\n",
    "    with open(os.path.join(\"preprocessed_cranfieldQueries\", filename), 'w') as file:\n",
    "        file.write(\" \".join(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f5fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize. Parameters are a BS text and tag\n",
    "def tokenize(file_soup_object, tag):\n",
    "    #extract informatio between TREC tags\n",
    "    TREC_data = file_soup_object.findAll(tag)\n",
    "    \n",
    "    # convert to string and remove tag related information from the string\n",
    "    TREC_data = ''.join(map(str, TREC_data))\n",
    "    TREC_data = TREC_data.replace(tag,\" \")\n",
    "    \n",
    "    # Convert string data to lower case, remove punctuations, stop wrods\n",
    "    TREC_data = TREC_data.lower()\n",
    "    TREC_data = TREC_data.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(TREC_data)\n",
    "   \n",
    "    # Tokenize  text\n",
    "    tokens = word_tokenize(TREC_data)\n",
    "\n",
    "    # Remove stop words\n",
    "    # get a list of stopwords to be used for pre-processing the Cranfield dataset\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stem the tokens\n",
    "    # Initialise a Porter Stemmer Object to Create a Tree/ Stem Model to scan through documents\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_tokens = [stemmer.stem(word) for word in clean_tokens]\n",
    "\n",
    "    # Convert list of tokens to string\n",
    "    clean_stem_tokens = ' '.join(stem_tokens)\n",
    "    \n",
    "    return clean_stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead239c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Process raw files and save it in new folder \"preprocessed_cranfieldDocs\"\n",
    "#file path for original Cranfield docs\n",
    "raw_file_path = 'cranfieldDocs'\n",
    "#file path for pre processed docs\n",
    "preprocessed_file_path = 'preprocessed_cranfieldDocs'\n",
    "#scan through each file name\n",
    "for file in file_names:\n",
    "    raw_file = raw_file_path+ \"/\" + file\n",
    "    preprocessed_file = preprocessed_file_path + \"/\" + file\n",
    "    \n",
    "    with open(raw_file) as r_file:\n",
    "        with open(preprocessed_file, 'w') as p_file:\n",
    "            # read information in each raw cranfield document\n",
    "            file_information = r_file.read()\n",
    "            #BeautifulSoup object to extract information between TREC tags\n",
    "            file_soup_object = BS(file_information)\n",
    "            \n",
    "            #extract docnofrom each raw file\n",
    "            doc_no = tokenize(file_soup_object, 'docno')\n",
    "            \n",
    "            #extract title from each raw file\n",
    "            title = tokenize(file_soup_object, 'title')\n",
    "            \n",
    "            #extract author number from each raw file\n",
    "            author = tokenize(file_soup_object, 'author')\n",
    "                        \n",
    "            #extract bibliography number from each raw file\n",
    "            biblio = tokenize(file_soup_object, 'biblio')\n",
    "                      \n",
    "            #extract text number from each raw file\n",
    "            text = tokenize(file_soup_object, 'text')\n",
    "            \n",
    "            # save tokeized infomation into new files in pre-procecessed directory\n",
    "            p_file.write(text)\n",
    "            p_file.write(\" \")\n",
    "        p_file.close()\n",
    "    r_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44643c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# iteration and run_id values are not relevant but necessary\n",
    "#for dyn_eval\n",
    "iteration = \"Q0\"\n",
    "run_id = \"140\"\n",
    "\n",
    "#return top 100 documents\n",
    "rank = 100\n",
    "\n",
    "# Define paths to the preprocessed files\n",
    "queries_path = \"preprocessed_cranfieldQueries\"\n",
    "docs_path = \"preprocessed_cranfieldDocs\"\n",
    "\n",
    "# Get file names of the queries and docs\n",
    "# Get results of all files\n",
    "queries_filenames = sorted(os.listdir(queries_path)) \n",
    "docs_filenames = sorted(os.listdir(docs_path))\n",
    "\n",
    "# Load the preprocessed queries and docs\n",
    "queries = []\n",
    "for filename in queries_filenames:\n",
    "    with open(os.path.join(queries_path, filename), 'r') as file:\n",
    "        queries.append(file.read())\n",
    "        \n",
    "docs = []\n",
    "for filename in docs_filenames:\n",
    "    with open(os.path.join(docs_path, filename), 'r') as file:\n",
    "        docs.append(file.read())\n",
    "\n",
    "# Compute the TF-IDF matrix for the queries and docs\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_queries = vectorizer.fit_transform(queries)\n",
    "tfidf_docs = vectorizer.transform(docs)\n",
    "\n",
    "# Calculate the cosine similarity between each query and each doc\n",
    "similarity_matrix = cosine_similarity(tfidf_queries, tfidf_docs)\n",
    "\n",
    "# Write the results to a text file\n",
    "with open(\"resultsCosineSimilarity.txt\", \"w\") as f:\n",
    "    for i, query_similarities in enumerate(similarity_matrix):\n",
    "        query_id = os.path.splitext(queries_filenames[i])[0]\n",
    "        top_matches = sorted(enumerate(query_similarities), key=lambda x: x[1], reverse=True)[:rank]\n",
    "        for j, (doc_index, similarity) in enumerate(top_matches):\n",
    "            doc_id = os.path.splitext(docs_filenames[doc_index])[0]\n",
    "            f.write(f\"{query_id} {iteration} {doc_id} {j+1} {similarity:.5f} {run_id}\\n\")\n",
    "            \n",
    "with open(\"resultsCosineSimilarity.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"resultsCosineSimilarity.txt\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # remove spaces in the first column\n",
    "        parts[0] = parts[0].replace(\" \", \"\")  \n",
    "        f.write(\" \".join(parts) + \"\\n\")\n",
    "        \n",
    "# Open the file in read mode and read the lines\n",
    "with open(\"resultsCosineSimilarity.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Sort the lines in ascending order of the first column\n",
    "sorted_lines = sorted(lines, key=lambda line: int(line.split()[0]))\n",
    "\n",
    "# Open the file in write mode and write the sorted lines\n",
    "with open(\"resultsCosineSimilarity.txt\", \"w\") as f:\n",
    "    f.writelines(sorted_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb000aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import os\n",
    "\n",
    "\n",
    "# iteration and run_id values are not relevant but necessary\n",
    "#for dyn_eval\n",
    "iteration = \"Q0\"\n",
    "run_id = \"140\"\n",
    "\n",
    "# Path to directories containing preprocessed files\n",
    "query_dir = \"preprocessed_cranfieldQueries\"\n",
    "doc_dir = \"preprocessed_cranfieldDocs\"\n",
    "\n",
    "# Collect all preprocessed queries and documents\n",
    "queries = []\n",
    "for filename in os.listdir(query_dir):\n",
    "    with open(os.path.join(query_dir, filename), \"r\") as f:\n",
    "        queries.append(f.read().split())\n",
    "\n",
    "docs = []\n",
    "for filename in os.listdir(doc_dir):\n",
    "    with open(os.path.join(doc_dir, filename), \"r\") as f:\n",
    "        docs.append(f.read().split())\n",
    "\n",
    "# Create BM25 object with default parameters\n",
    "bm25 = BM25Okapi(docs, k1=1.2, b=0.75)\n",
    "\n",
    "# Compute similarities between queries and documents\n",
    "similarities = []\n",
    "for query in queries:\n",
    "    query_scores = bm25.get_scores(query)\n",
    "    similarities.append([(score, i) for i, score in enumerate(query_scores)])\n",
    "\n",
    "# Sort similarities in descending order and save top 100 ranks to file\n",
    "with open(\"resultsBM25.txt\", \"w\") as f:\n",
    "    for i, sim in enumerate(similarities):\n",
    "        ranked_sim = sorted(sim, reverse=True)[:100] # only top 100 ranks\n",
    "        for rank, (score, doc_index) in enumerate(ranked_sim):\n",
    "            doc_id = os.path.splitext(os.listdir(doc_dir)[doc_index])[0]\n",
    "            f.write(f\"{os.path.splitext(os.listdir(query_dir)[i])[0]} {iteration} {doc_id} {rank+1} {score:.5f} {run_id}\\n\")\n",
    "            \n",
    "with open(\"resultsBM25.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"resultsBM25.txt\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        parts[0] = parts[0].replace(\" \", \"\")  # remove spaces in the first column\n",
    "        f.write(\" \".join(parts) + \"\\n\")\n",
    "        \n",
    "# Open the file in read mode and read the lines\n",
    "with open(\"resultsBM25.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Sort the lines in ascending order of the first column\n",
    "sorted_lines = sorted(lines, key=lambda line: int(line.split()[0]))\n",
    "\n",
    "# Open the file in write mode and write the sorted lines\n",
    "with open(\"resultsBM25.txt\", \"w\") as f:\n",
    "    f.writelines(sorted_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637fc350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim import corpora, similarities\n",
    "\n",
    "\n",
    "# iteration and run_id values are not relevant but necessary\n",
    "#for dyn_eval\n",
    "iteration = \"Q0\"\n",
    "rank =100\n",
    "run_id = \"140\"\n",
    "\n",
    "# Path to directories containing preprocessed files\n",
    "query_dir = \"preprocessed_cranfieldQueries\"\n",
    "doc_dir = \"preprocessed_cranfieldDocs\"\n",
    "\n",
    "# Collect all preprocessed queries and documents\n",
    "queries = []\n",
    "for filename in os.listdir(query_dir):\n",
    "    with open(os.path.join(query_dir, filename), \"r\") as f:\n",
    "        queries.append(f.read().split())\n",
    "\n",
    "docs = []\n",
    "for filename in os.listdir(doc_dir):\n",
    "    with open(os.path.join(doc_dir, filename), \"r\") as f:\n",
    "        docs.append(f.read().split())\n",
    "\n",
    "# Build dictionary from documents\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "# Convert documents into bag-of-words format and index them\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "index = similarities.MatrixSimilarity(corpus)\n",
    "\n",
    "# Process queries and retrieve top 100 similar documents for each query\n",
    "similarities = []\n",
    "for query in queries:\n",
    "    query_vec = dictionary.doc2bow(query)\n",
    "    sims = index[query_vec]\n",
    "    similarities.append([(score, i) for i, score in enumerate(sims)])\n",
    "\n",
    "    \n",
    "# Sort similarities in descending order and save top 100 ranks to file\n",
    "with open(\"resultsGensim.txt\", \"w\") as f:\n",
    "    for i, sim in enumerate(similarities):\n",
    "        ranked_sim = sorted(sim, reverse=True)[:100] # only top 100 ranks\n",
    "        for rank, (score, doc_index) in enumerate(ranked_sim):\n",
    "            doc_id = os.path.splitext(os.listdir(doc_dir)[doc_index])[0]\n",
    "            f.write(f\"{os.path.splitext(os.listdir(query_dir)[i])[0]} {iteration} {doc_id} {rank+1} {score:.5f} {run_id}\\n\")\n",
    "\n",
    "with open(\"resultsGensim.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"resultsGensim.txt\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        parts[0] = parts[0].replace(\" \", \"\")  # remove spaces in the first column\n",
    "        f.write(\" \".join(parts) + \"\\n\")\n",
    "        \n",
    "# Open the file in read mode and read the lines\n",
    "with open(\"resultsGensim.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Sort the lines in ascending order of the first column\n",
    "sorted_lines = sorted(lines, key=lambda line: int(line.split()[0]))\n",
    "\n",
    "# Open the file in write mode and write the sorted lines\n",
    "with open(\"resultsGensim.txt\", \"w\") as f:\n",
    "    f.writelines(sorted_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4cac68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Execute dyn_eval to run Dyneval and extract desired metrics MAP, P@5, and NDCG\n",
    "os.system('./dyneval qrel resultsCosineSimilarity.txt | grep -E \"(map|P_5 |ndcg)\" > cosine_eval.txt')\n",
    "os.system('./dyneval qrel resultsBM25.txt | grep -E \"(map|P_5 |ndcg)\" > bm25_eval.txt')\n",
    "os.system('./dyneval qrel resultsGensim.txt | grep -E \"(map|P_5 |ndcg)\" > gensim_eval.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
